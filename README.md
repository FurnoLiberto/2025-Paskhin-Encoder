# Энкодер для верификации диктора

Этот проект реализует обучение нейросетевого энкодера для задачи верификации диктора. 

Модель обучается извлекать векторы признаков (эмбеддинги) из аудиозаписей голоса с использованием функции потерь Triplet Loss.

Цель обучения — минимизировать расстояние между эмбеддингами одного и того же диктора и максимизировать расстояние между эмбеддингами разных дикторов.

- **Датасет**: CSTR VCTK Corpus
- **Признаки**: Мел-спектрограммы
- **Архитектура**: CNN
- **Функция потерь**: Triplet Margin Loss

## Установка и запуск

1.  Клонируйте репозиторий:
    ```bash
    git clone https://github.com/FurnoLiberto/2025-Paskhin-Encoder.git
    cd 2025-Paskhin-Encoder
    ```
2.  Установите зависимости:
    ```bash
    pip install torch torchaudio numpy
    ```
3.  Загрузите датасет:
    ```bash
    # Внимание: датасет весит ~11GB, загрузка может занять время.
    wget https://datashare.ed.ac.uk/bitstream/handle/10283/2651/VCTK-Corpus.zip
    unzip -q VCTK-Corpus.zip -d data/
    ```

4.  Запустите обучение:
    ```bash
    python src/train.py
    ```

5.  Запустите инференс:
    ```bash
    python src/inference.py
    ```

## Результаты обучения
Обучение проводилось в течение 5 эпох.
Ниже представлены графики обучения, полученные с помощью TensorBoard.
<img width="949" height="377" alt="изображение" src="https://github.com/user-attachments/assets/79935704-6271-4b0b-8a5d-73b812e088a4" />

<img width="943" height="382" alt="изображение" src="https://github.com/user-attachments/assets/8aebb5b5-d743-468d-9485-b663790495f0" />

- Как видно из графиков, и тренировочная (Loss/train), и валидационная (Loss/validation) функции потерь стабильно снижались на протяжении всего процесса обучения.
- Кривые потерь на обучающей и валидационной выборках идут очень близко друг к другу, что свидетельствует об отсутствии переобучения. Модель хорошо обобщается на новые данные.
- Итоговое значение потерь на обеих выборках составило ~0.056, что является отличным показателем сходимости модели.

Для практической проверки использовался скрипт inference.py, который вычисляет косинусное расстояние между эмбеддингами двух аудиофайлов. Порог для принятия решения был установлен на 0.7.
Сравнение двух файлов одного диктора:
Косинусное расстояние: 0.0523
Результат: SAME speaker (тот же диктор)
Сравнение двух файлов разных дикторов:
Косинусное расстояние: 0.7967
Результат: DIFFERENT speakers (разные дикторы)
### Выводы

- Модель успешно обучается, так как и тренировочная, и валидационная потери снижаются на протяжении N эпох.
- Наблюдается небольшое переобучение после X эпохи, что говорит о возможном потенциале для регуляризации (например, Dropout).
- Полученная модель способна различать дикторов: косинусное расстояние для записей одного диктора в среднем составляет ~0.2, а для разных дикторов ~0.9 (подставьте свои цифры).

## Неудачные эксперименты

- **Эксперимент**: Использование RNN (LSTM) архитектуры.
- **Результат**: Обучение было очень медленным, а лосс сходился хуже.
- **Вывод**: Для обработки спектрограмм как изображений CNN-архитектуры оказались более эффективными и быстрыми. Возможно, стоило бы попробовать другую предобработку данных для RNN или более сложную архитектуру.
