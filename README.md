# 2025-Paskhin-Encoder
# Speaker Verification Encoder

Этот проект реализует обучение нейросетевого энкодера для задачи верификации диктора с использованием Triplet Loss.

## Описание

Модель обучается извлекать векторы признаков (эмбеддинги) из аудиозаписей голоса. Цель обучения — минимизировать расстояние между эмбеддингами одного и того же диктора и максимизировать расстояние между эмбеддингами разных дикторов.

- **Датасет**: CSTR VCTK Corpus
- **Признаки**: Мел-спектрограммы
- **Архитектура**: CNN
- **Функция потерь**: Triplet Margin Loss

## Установка и запуск

1.  Клонируйте репозиторий:
    ```bash
    git clone ...
    cd speaker-verification
    ```
2.  Установите зависимости:
    ```bash
    pip install -r requirements.txt
    ```
3.  Запустите обучение:
    ```bash
    python src/train.py
    ```

## Результаты обучения

Ниже представлены графики обучения, полученные с помощью TensorBoard.
<img width="949" height="377" alt="изображение" src="https://github.com/user-attachments/assets/79935704-6271-4b0b-8a5d-73b812e088a4" />

*(Вставьте сюда скриншоты графиков `train_loss` и `val_loss` из W&B)*

![Loss Chart](link_to_your_chart_image.png)

### Выводы

- Модель успешно обучается, так как и тренировочная, и валидационная потери снижаются на протяжении N эпох.
- Наблюдается небольшое переобучение после X эпохи, что говорит о возможном потенциале для регуляризации (например, Dropout).
- Полученная модель способна различать дикторов: косинусное расстояние для записей одного диктора в среднем составляет ~0.2, а для разных дикторов ~0.9 (подставьте свои цифры).

## Неудачные эксперименты

- **Эксперимент**: Использование RNN (LSTM) архитектуры.
- **Результат**: Обучение было очень медленным, а лосс сходился хуже.
- **Вывод**: Для обработки спектрограмм как изображений CNN-архитектуры оказались более эффективными и быстрыми. Возможно, стоило бы попробовать другую предобработку данных для RNN или более сложную архитектуру.
