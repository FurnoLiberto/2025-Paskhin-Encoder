# Энкодер для верификации диктора

Этот проект реализует обучение нейросетевого энкодера для задачи верификации диктора. 

Модель обучается извлекать векторы признаков (эмбеддинги) из аудиозаписей голоса с использованием функции потерь Triplet Loss.

Цель обучения — минимизировать расстояние между эмбеддингами одного и того же диктора и максимизировать расстояние между эмбеддингами разных дикторов.

- **Датасет**: CSTR VCTK Corpus
- **Признаки**: Мел-спектрограммы
- **Архитектура**: CNN
- **Функция потерь**: Triplet Margin Loss

## Установка и запуск

1.  Клонируйте репозиторий:
    ```bash
    git clone https://github.com/FurnoLiberto/2025-Paskhin-Encoder.git
    cd 2025-Paskhin-Encoder
    ```
2.  Установите зависимости:
    ```bash
    pip install torch torchaudio numpy
    ```
3.  Загрузите датасет:
    ```bash
    # Внимание: датасет весит ~11GB, загрузка может занять время.
    wget https://datashare.ed.ac.uk/bitstream/handle/10283/2651/VCTK-Corpus.zip
    unzip -q VCTK-Corpus.zip -d data/
    ```

4.  Запустите обучение:
    ```bash
    python src/train.py
    ```

5.  Запустите инференс:
    ```bash
    python src/inference.py
    ```
## Архитектура

Слой	Входной размер/каналы	Выходной размер/каналы	Назначение
Input	(1, 128, 282)	(1, 128, 282)	Исходная мел-спектрограмма
Conv1	1	32	Поиск 32 типов базовых признаков (линии, углы)
Pool1	(32, 128, 282)	(32, 64, 141)	Обобщение, уменьшение размерности
Conv2	32	64	Поиск 64 типов более сложных текстур
Pool2	(64, 64, 141)	(64, 32, 70)	Обобщение, уменьшение размерности
Conv3	64	128	Поиск 128 типов высокоуровневых паттернов
Pool3	(128, 32, 70)	(128, 16, 35)	Финальное обобщение
Flatten	(128, 16, 35)	71680	Преобразование в вектор
FC1	71680	512	Анализ комбинаций признаков
FC2	512	128	Финальный эмбеддинг


## Результаты обучения
Обучение проводилось в течение 5 эпох.
Ниже представлены графики обучения, полученные с помощью TensorBoard.
<img width="949" height="377" alt="изображение" src="https://github.com/user-attachments/assets/79935704-6271-4b0b-8a5d-73b812e088a4" />

<img width="943" height="382" alt="изображение" src="https://github.com/user-attachments/assets/8aebb5b5-d743-468d-9485-b663790495f0" />

- Как видно из графиков, и тренировочная (Loss/train), и валидационная (Loss/validation) функции потерь стабильно снижались на протяжении всего процесса обучения.
- Кривые потерь на обучающей и валидационной выборках идут очень близко друг к другу, что свидетельствует об отсутствии переобучения. Модель хорошо обобщается на новые данные.
- Итоговое значение потерь на обеих выборках составило ~0.056, что является отличным показателем сходимости модели.

Для практической проверки использовался скрипт inference.py, который вычисляет косинусное расстояние между эмбеддингами двух аудиофайлов. Порог для принятия решения был установлен на 0.7.

Сравнение двух файлов одного диктора:
- Косинусное расстояние: 0.0523
- Результат: SAME speaker (тот же диктор)
Сравнение двух файлов разных дикторов:

- Косинусное расстояние: 0.7967
- Результат: DIFFERENT speakers (разные дикторы)
- 
### Выводы

- Выбранная CNN-архитектура и функция потерь Triplet Loss оказались эффективными для решения задачи верификации диктора.
- Модель успешно обучилась, что подтверждается как стабильным снижением потерь на графиках, так и отсутствием переобучения.
- Результаты инференса демонстрируют, что энкодер генерирует семантически верные эмбеддинги: очень близкие для одного диктора (~0.05) и далекие для разных (~0.8), что позволяет уверенно их различать.
